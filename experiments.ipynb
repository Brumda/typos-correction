{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:10:53.578542Z",
     "start_time": "2025-02-25T23:10:53.575565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "import psutil\n",
    "import torch\n",
    "import numpy as np\n",
    "import tracemalloc\n",
    "from dataclasses import dataclass\n",
    "from contextlib import contextmanager\n",
    "from typing import Callable, Any\n",
    "import gc"
   ],
   "id": "d950409f63a04853",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:10:54.424755Z",
     "start_time": "2025-02-25T23:10:54.421199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    model_name: str\n",
    "    model_size: float\n",
    "    inference_time_ms: float\n",
    "    peak_memory_mb: float\n",
    "    gpu_memory_mb: float\n",
    "    throughput: float\n",
    "    accuracy: float"
   ],
   "id": "5c38edb7ddfc4612",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T19:31:54.187711Z",
     "start_time": "2025-02-25T19:31:54.179335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ModelBenchmark:\n",
    "    def __init__(self, device: str = 'cuda'):\n",
    "        self.device = device\n",
    "\n",
    "    @contextmanager\n",
    "    def _measure_memory(self):\n",
    "        tracemalloc.start()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            self.peak_memory = peak / 1024 ** 2\n",
    "\n",
    "    def _get_gpu_memory(self) -> float:\n",
    "        if self.device == 'cuda':\n",
    "            return torch.cuda.max_memory_allocated() / 1024 ** 2\n",
    "        return 0\n",
    "\n",
    "    def _clear_memory(self):\n",
    "        gc.collect()\n",
    "        if self.device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def _get_model_size(self, model) -> float:\n",
    "        \"\"\"Pytorch models only\"\"\"\n",
    "        if self.device == 'cuda':\n",
    "            param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "            buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "            total_size_mb = (param_size + buffer_size) / 1024 ** 2\n",
    "            return total_size_mb\n",
    "        return 0\n",
    "\n",
    "    def benchmark_model(self,\n",
    "                        model,\n",
    "                        input_texts: list[str],\n",
    "                        true_texts: list[str],\n",
    "                        model_name: str,\n",
    "                        predict: Callable[[Any, str], str],\n",
    "                        warm_up_runs: int = 3,\n",
    "                        num_runs: int = 5\n",
    "                        ) -> BenchmarkResult:\n",
    "\n",
    "        print(f\"Starting {warm_up_runs} warm-up iterations for {model_name}...\")\n",
    "        start = time.time()\n",
    "        for _ in range(warm_up_runs):\n",
    "            for text in input_texts[:len(input_texts) / 2]:\n",
    "                _ = predict(model, text)\n",
    "        print(f\"Finished warm-up after {time.time() - start} seconds.\")\n",
    "\n",
    "        inference_times = []\n",
    "        throughputs = []\n",
    "        memory_usages = []\n",
    "        gpu_memory_usages = []\n",
    "\n",
    "        print(f\"Starting benchmark iterations...\")\n",
    "        for run in range(num_runs):\n",
    "            self._clear_memory()\n",
    "            with self._measure_memory():\n",
    "                total_tokens = 0\n",
    "                outputs = []\n",
    "                start_time = time.time()\n",
    "\n",
    "                for text in input_texts:\n",
    "                    outputs.append(predict(model, text))\n",
    "\n",
    "                    total_tokens += len(text.split())\n",
    "\n",
    "                inference_time = time.time() - start_time\n",
    "                # TODO get accuracy\n",
    "                throughputs.append(total_tokens / inference_time)\n",
    "                inference_times.append(inference_time)\n",
    "                memory_usages.append(self.peak_memory)\n",
    "                gpu_memory_usages.append(self._get_gpu_memory())\n",
    "\n",
    "            print(f\"Finished {run}/{num_runs} iteration in {inference_time} seconds.\")\n",
    "\n",
    "        avg_inference_time = np.mean(inference_times)\n",
    "        avg_throughput = np.mean(throughputs)\n",
    "        avg_memory = np.mean(memory_usages)\n",
    "        avg_gpu_memory = np.mean(gpu_memory_usages)\n",
    "\n",
    "        return BenchmarkResult(model_name=model_name,\n",
    "                               model_size=self._get_model_size(model),\n",
    "                               inference_time_ms=avg_inference_time,\n",
    "                               peak_memory_mb=avg_memory,\n",
    "                               gpu_memory_mb=avg_gpu_memory,\n",
    "                               throughput=avg_throughput)"
   ],
   "id": "df5a80c1c4074048",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Neuspell\n",
   "id": "d82ea1dbdbf90514"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T19:31:56.063634Z",
     "start_time": "2025-02-25T19:31:54.193052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from neuspell import BertChecker\n",
    "\n",
    "checker = BertChecker(device='cuda')\n",
    "checker.from_pretrained()"
   ],
   "id": "35a00e77b0f9e7e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocab from path:C:\\FIT\\bakalarka\\.venv\\Lib\\site-packages\\neuspell\\../neuspell_data/checkpoints/subwordbert-probwordnoise\\vocab.pkl\n",
      "initializing model\n",
      "loading pretrained weights from path:C:\\FIT\\bakalarka\\.venv\\Lib\\site-packages\\neuspell\\../neuspell_data/checkpoints/subwordbert-probwordnoise\n",
      "Loading model params from checkpoint dir: C:\\FIT\\bakalarka\\.venv\\Lib\\site-packages\\neuspell\\../neuspell_data/checkpoints/subwordbert-probwordnoise\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T19:39:16.196750Z",
     "start_time": "2025-02-25T19:39:16.193366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "with open(DATA_PATH + \"small_clean.txt\", 'r', encoding='utf-8', newline='\\n') as f:\n",
    "    input_clean = [line.strip() for line in f if line != \"\"]\n",
    "\n",
    "with open(DATA_PATH + \"small_corrupt.txt\", 'r', encoding='utf-8', newline='\\n') as f:\n",
    "    input_corrupt = [line.strip() for line in f if line != \"\"]"
   ],
   "id": "e14ef88330d6455d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "286a480c9c4eee11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T19:43:02.835487Z",
     "start_time": "2025-02-25T19:43:02.832397Z"
    }
   },
   "cell_type": "code",
   "source": "benchmark = ModelBenchmark()",
   "id": "fb67e605e9ba6b5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "benchmark.benchmark_model(checker, input_clean, input_corrupt, \"neuspell-bert\", lambda x: checker.correct_string(x), )",
   "id": "4ede1f5ccce2ed7a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
